{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "np.random.seed(0) \n",
    "\n",
    "m = 10  \n",
    "n = 784 \n",
    "lambda_ = 1e-5  # regularization parameter\n",
    "\n",
    "true_x = np.array(Image.open('./data/4.jpg'))\n",
    "true_x = true_x.reshape(784)\n",
    "\n",
    "x=true_x+np.random.randn(784)*(2.4e-3)\n",
    "\n",
    "O = np.random.randn(n, 100, 784)\n",
    "A = [O[i] for i in range(m)]\n",
    "y = [A[i] @ true_x for i in range(m)]\n",
    "\n",
    "'''\n",
    "Constant error setting:C=3 or 5 and Diminishing errors setting:C=20 or 30 and No error setting:C=0\n",
    "Gaussian vector  e=C*r or C*r**t\n",
    "'''\n",
    "r=np.random.normal(loc=0, scale=1e-3, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pg_algorithms:\n",
    "\n",
    "    def soft_thresholding(self, x, threshold):\n",
    "\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, A, x, y):\n",
    "\n",
    "        grad = np.zeros_like(x)\n",
    "        n = len(y)\n",
    "        for i in range(n):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += (A[i].T @ residual) / n\n",
    "        return grad\n",
    "\n",
    "    def proximal_gradient_descent(self, x, A, y, lambda_, true_x, r, max_iter, p, alpha, C): \n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm(((x0-true_x)))**2)/len(y0))\n",
    "\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "\n",
    "            # y0+=C*(r**(iteration+1))\n",
    "            y0+=C*r\n",
    "\n",
    "            grad = self.gradient_smooth_part(A, x0, y0)\n",
    "            x_new = self.soft_thresholding(x0 - alpha * grad, alpha * lambda_)\n",
    "            x0 = x_new\n",
    "            \n",
    "            current_err = (np.linalg.norm(((x0-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "        \n",
    "            y0=y.copy()\n",
    "\n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-PG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bpg_algorithms:\n",
    "    def gradient_of_smooth_part(self, x, y, A):\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(y)):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += 2 * A[i].T @ residual\n",
    "        return grad / len(y)\n",
    "\n",
    "    def soft_thresholding(self, x, lambda_):\n",
    "\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)\n",
    "\n",
    "    def block_proximal_gradient(self, x, A, y, lambda_, true_x, r, max_iter, p, learning_rate, C): \n",
    "\n",
    "        x0=x.copy()\n",
    "        y0=y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm(((x0-true_x)))**2)/len(y))\n",
    "        for iteration in range(max_iter):\n",
    "\n",
    "            # y0+=C*(r**(iteration+1))\n",
    "            y0+=C*r\n",
    "\n",
    "            grad = self.gradient_of_smooth_part(x0, y0, A)\n",
    "            \n",
    "            x_new = x0 - learning_rate * grad\n",
    "            \n",
    "            x_new = self.soft_thresholding(x_new, lambda_ * learning_rate)\n",
    "            x0 = x_new\n",
    "            \n",
    "            # Check convergence\n",
    "            current_err = (np.linalg.norm(((x0-true_x)))**2)/len(y)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "            \n",
    "            y0=y.copy()\n",
    "            \n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class spg_algorithm:\n",
    "\n",
    "    def prox_l1(self, v, lambda_):\n",
    "        return np.sign(v) * np.maximum(np.abs(v) - lambda_, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, x, A, y, idx):\n",
    "        return 2 *A[idx].T @ (A[idx] @ x - y[idx]) / len(y)\n",
    "\n",
    "    def run_spg(self, x, A, y, lambda_, true_x, r, num_iterations, p, step_size, C): \n",
    "\n",
    "        x0=x.copy()\n",
    "        y0=y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm(((x0-true_x)))**2)/len(y))\n",
    "        for iteration in range(num_iterations):\n",
    "\n",
    "            # y0+=C*(r**(iteration+1))\n",
    "            y0+=C*r\n",
    "\n",
    "            idx = np.random.randint(len(y))\n",
    "            \n",
    "            grad_smooth = self.gradient_smooth_part(x0, A, y0, idx)\n",
    "            \n",
    "            x_temp = x0 - step_size * grad_smooth\n",
    "            \n",
    "            x0 = self.prox_l1(x_temp, step_size * lambda_)\n",
    "            \n",
    "            current_err = (np.linalg.norm(((x0-true_x)))**2)/len(y)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "        \n",
    "            y0=y.copy()\n",
    "\n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMN Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import cg\n",
    "import math\n",
    "\n",
    "class admm_algorithm:\n",
    "    \n",
    "    def soft_threshold(self, x, threshold):\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def admm_solver(self, x, A, y, lambda_, true_x, r, max_iter, p, rho, C):\n",
    "\n",
    "        x0=x.copy()\n",
    "        y0=y.copy()\n",
    "\n",
    "        m, n = len(y0),len(x0)\n",
    "\n",
    "        z = x.copy()\n",
    "        u = np.zeros(n)\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm(((x0-true_x)))**2)/len(y0)) \n",
    "\n",
    "        \n",
    "        AtA = sum([A[i].T @ A[i] for i in range(m)]) / m + rho * np.eye(n)\n",
    "        b1 = sum([A[i].T @ y0[i] for i in range(m)]) / m\n",
    "\n",
    "        for iteration in range(max_iter):\n",
    "\n",
    "            y0+=C*r\n",
    "            # y0+=C*(r**(iteration+1))\n",
    "\n",
    "            b = b1 + rho * (z - u)\n",
    "            x0, _ = cg(AtA, b, x0, maxiter=10, rtol=(1e-4/(iteration/2+1)/math.pow(0.999,(iteration+1)/3e2)*0.75 + 1e-9))\n",
    "        \n",
    "            z = self.soft_threshold(x0 + u, lambda_ / rho)\n",
    "\n",
    "            u = u + x0 - z\n",
    "\n",
    "            current_err = (np.linalg.norm(((x0-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "\n",
    "            y0=y.copy()\n",
    "\n",
    "        return x0,current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGRR Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "class pgrr_algorithm:\n",
    "    def soft_thresholding(self, x: np.ndarray, threshold: float) -> np.ndarray:\n",
    "        \n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def compute_gradient(self, x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:\n",
    "        \n",
    "        n = len(y)\n",
    "        gradient = np.zeros_like(x)\n",
    "        for i in range(n):\n",
    "            gradient += 2 * A[i].T @ (A[i] @ x - y[i])\n",
    "        return gradient / n\n",
    "\n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, num_epochs, p, gamma, C):\n",
    "       \n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        y0=y.copy()\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm(((x-true_x)))**2)/len(y))\n",
    "\n",
    "        for iteration in range(num_epochs):\n",
    "\n",
    "            # y0+=C*(r**(iteration+1))\n",
    "            y0+=C*r\n",
    "\n",
    "            for i in np.random.permutation(n):\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                x = self.soft_thresholding(x - gamma * gradient, gamma * lambda_)\n",
    "            \n",
    "            current_err = (np.linalg.norm(((x-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "            \n",
    "            y0=y.copy()\n",
    "        \n",
    "        return x,current_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ours Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "class ours_algorithms:\n",
    "\n",
    "    def soft_thresholding(self ,x, threshold):\n",
    "       \n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "    \n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, num_epochs, p, gamma, C, momentum):\n",
    "\n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        velocity = np.zeros_like(x)\n",
    "        y0=y.copy()\n",
    "\n",
    "        current_errs=[]\n",
    "        current_errs.append((np.linalg.norm(((x-true_x)))**2)/len(y))\n",
    "        \n",
    "        \n",
    "        for iteration in range(num_epochs):\n",
    "\n",
    "            # y0+=C*(r**(iteration+1))\n",
    "            y0+=C*r\n",
    "\n",
    "            for i in np.random.permutation(n):\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                velocity = momentum * velocity + gamma * gradient\n",
    "                x = self.soft_thresholding(x - velocity, gamma * lambda_)\n",
    "        \n",
    "            current_err = (np.linalg.norm(((x-true_x)))**2)/len(y0)\n",
    "            if (iteration!=0) and (iteration % p == 0):\n",
    "                current_errs.append(current_err)\n",
    "                print(f\"Iteration {iteration}, Objective Value: {current_err}\")\n",
    "                \n",
    "            y0=y.copy()\n",
    "\n",
    "        return x,current_errs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PG algorithm\n",
    "pg=pg_algorithms()\n",
    "\n",
    "# B-PG algorithm\n",
    "bpg = bpg_algorithms()\n",
    "\n",
    "# SPG algorithm\n",
    "spg = spg_algorithm()\n",
    "\n",
    "# ADMM algorithm\n",
    "admm = admm_algorithm()\n",
    "\n",
    "# PGRR algorithm\n",
    "pgrr=pgrr_algorithm()\n",
    "\n",
    "# ours algorithm\n",
    "ours = ours_algorithms()\n",
    "\n",
    "\n",
    "C=5\n",
    "p=5000\n",
    "all_iteration=int(1.45e5+1)\n",
    "\n",
    "pg_x,pg_errors= pg.proximal_gradient_descent(x, A, y, lambda_, true_x, r, all_iteration, p, 2e-7, C)\n",
    "bpg_x,bpg_errors = bpg.block_proximal_gradient(x, A, y, lambda_, true_x, r, all_iteration, p, 1.1e-7, C)\n",
    "spg_x,spg_errors= spg.run_spg(x, A, y, lambda_, true_x, r, all_iteration, p, 1.4e-6, C)\n",
    "admm_x,admm_errors = admm.admm_solver(x, A, y, lambda_, true_x, r, all_iteration, p, 1e5, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr.PG_RR(x, A, y, lambda_, true_x, r, all_iteration, p, 6.5e-8, C)\n",
    "ours_x,ours_errors= ours.PG_RR(x, A, y, lambda_, true_x, r, all_iteration, p, 6.5e-8, C, 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('', np.array([pg_errors,bpg_errors,spg_errors,admm_errors,pg_rr_errors,ours_errors]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ael",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
