{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "np.random.seed(0) \n",
    "\n",
    "m = 10  \n",
    "n = 784 \n",
    "lambda_ = 1e-5  # regularization parameter\n",
    "\n",
    "true_x = np.array(Image.open('./data/4.jpg'))\n",
    "true_x = true_x.reshape(784)\n",
    "\n",
    "x=true_x+np.random.randn(784)*(2.4e-3)\n",
    "\n",
    "O = np.random.randn(n, 100, 784)\n",
    "A = [O[i] for i in range(m)]\n",
    "y = [A[i] @ true_x for i in range(m)]\n",
    "\n",
    "'''\n",
    "Constant error setting:C=3 or 5 and Diminishing errors setting:C=20 or 30 and No error setting:C=0\n",
    "Gaussian vector  e=C*r or C*r**t\n",
    "'''\n",
    "r=np.random.normal(loc=0, scale=1e-3, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class pg_algorithms_time:\n",
    "\n",
    "    def soft_thresholding(self, x, threshold):\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, A, x, y):\n",
    "        grad = np.zeros_like(x)\n",
    "        n = len(y)\n",
    "        for i in range(n):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += (A[i].T @ residual) / n\n",
    "        return grad\n",
    "\n",
    "    def proximal_gradient_descent(self, x, A, y, lambda_, true_x, r, max_duration, alpha, C): \n",
    "\n",
    "        start_time = time.time()  \n",
    "        current_time = start_time \n",
    "        iterations = 0  \n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  \n",
    "            #y0+=C*(r**(iterations+1))\n",
    "            y0 += C*r\n",
    "\n",
    "            grad = self.gradient_smooth_part(A, x0, y0)\n",
    "            x_new = self.soft_thresholding(x0 - alpha * grad, alpha * lambda_)\n",
    "            x0 = x_new\n",
    "\n",
    "            iterations += 1\n",
    "   \n",
    "            y0 = y.copy()\n",
    "            current_time = time.time()  \n",
    "\n",
    "        return x0, (np.linalg.norm(((x0-true_x)))**2)/len(y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-PG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class bpg_algorithms_time:\n",
    "\n",
    "    def gradient_of_smooth_part(self, x, y, A):\n",
    "        grad = np.zeros_like(x)\n",
    "        for i in range(len(y)):\n",
    "            residual = A[i] @ x - y[i]\n",
    "            grad += 2 * A[i].T @ residual\n",
    "        return grad / len(y)\n",
    "\n",
    "    def soft_thresholding(self, x, lambda_):\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)\n",
    "\n",
    "    def block_proximal_gradient(self, x, A, y, lambda_, true_x, r, max_duration, learning_rate, C): \n",
    "\n",
    "        start_time = time.time()  \n",
    "        current_time = start_time\n",
    "        iterations = 0  \n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  \n",
    "            \n",
    "            # y0+=C*(r**(iterations+1))\n",
    "            y0 += C*r\n",
    "\n",
    "            grad = self.gradient_of_smooth_part(x0, y0, A)\n",
    "            \n",
    "            x_new = x0 - learning_rate * grad\n",
    "            \n",
    "            x_new = self.soft_thresholding(x_new, lambda_ * learning_rate)\n",
    "            x0 = x_new\n",
    "            \n",
    "            y0 = y.copy()\n",
    "            iterations += 1 \n",
    "            current_time = time.time()\n",
    "\n",
    "        return x0,(np.linalg.norm(((x0-true_x)))**2)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPG Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class spg_algorithm_time:\n",
    "\n",
    "    def prox_l1(self, v, lambda_):\n",
    "\n",
    "        return np.sign(v) * np.maximum(np.abs(v) - lambda_, 0)\n",
    "\n",
    "    def gradient_smooth_part(self, x, A, y, idx):\n",
    "\n",
    "        return 2 * A[idx].T @ (A[idx] @ x - y[idx]) / len(y)\n",
    "\n",
    "    def run_spg(self, x, A, y, lambda_, true_x, r, max_duration, step_size, C): \n",
    "\n",
    "        start_time = time.time()  \n",
    "        current_time = start_time\n",
    "        iterations = 0 \n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  \n",
    "            # y0+=C*(r**(iterations+1))\n",
    "            y0 += C*r\n",
    "\n",
    "            idx = np.random.randint(len(y))\n",
    "            \n",
    "            grad_smooth = self.gradient_smooth_part(x0, A, y0, idx)\n",
    "            \n",
    "            x_temp = x0 - step_size * grad_smooth\n",
    "            \n",
    "            x0 = self.prox_l1(x_temp, step_size * lambda_)\n",
    "                                    \n",
    "            y0 = y.copy()\n",
    "            iterations += 1  \n",
    "            current_time = time.time()\n",
    "\n",
    "        return x0, (np.linalg.norm(((x0-true_x)))**2)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMN Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse.linalg import cg\n",
    "import math\n",
    "\n",
    "class admm_algorithm_time:\n",
    "    \n",
    "    def soft_threshold(self, x, threshold):\n",
    "\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def admm_solver(self, x, A, y, lambda_, true_x, r, max_duration, rho, C):\n",
    "\n",
    "        start_time = time.time()  \n",
    "        current_time = start_time\n",
    "        iterations = 0  \n",
    "\n",
    "        x0 = x.copy()\n",
    "        y0 = y.copy()\n",
    "\n",
    "        m, n = len(y0), len(x0)\n",
    "        \n",
    "        z = x.copy()\n",
    "        u = np.zeros(n)\n",
    "\n",
    "\n",
    "        AtA = sum([A[i].T @ A[i] for i in range(m)]) / m + rho * np.eye(n)\n",
    "        b1 = sum([A[i].T @ y0[i] for i in range(m)]) / m\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  \n",
    "            \n",
    "            #y0+=C*(r**(iterations+1))\n",
    "            y0+=C*r\n",
    "\n",
    "            b = b1 + rho * (z - u)\n",
    "            x0, _ = cg(AtA, b, x0, maxiter=10, rtol=(1e-4/(iterations/2+1)/math.pow(0.999,(iterations+1)/3e2)*0.75 + 1e-9))\n",
    "        \n",
    "            z = self.soft_threshold(x0 + u, lambda_ / rho)\n",
    "\n",
    "            u = u + x0 - z\n",
    "\n",
    "            y0 = y.copy()\n",
    "            iterations += 1  \n",
    "            current_time = time.time()\n",
    "            \n",
    "        return x0, (np.linalg.norm(((x0-true_x)))**2)/len(y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGRR Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import time\n",
    "\n",
    "class pgrr_algorithm_time:\n",
    "    def soft_thresholding(self, x: np.ndarray, threshold: float) -> np.ndarray:\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def compute_gradient(self, x: np.ndarray, A: List[np.ndarray], y: List[np.ndarray]) -> np.ndarray:\n",
    "        n = len(y)\n",
    "        gradient = np.zeros_like(x)\n",
    "        for i in range(n):\n",
    "            gradient += 2 * A[i].T @ (A[i] @ x - y[i])\n",
    "        return gradient / n\n",
    "\n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, max_duration, gamma, C):\n",
    "        start_time = time.time()  \n",
    "        current_time = start_time\n",
    "        iterations = 0  \n",
    "\n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        y0 = y.copy()\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  \n",
    "            \n",
    "            y0 += C*r\n",
    "            # y0+=C*(r**(iterations+1))\n",
    "            permuted_indices = np.random.permutation(n)\n",
    "            \n",
    "            for i in permuted_indices:\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                x = self.soft_thresholding(x - gamma * gradient, gamma * lambda_)\n",
    "                \n",
    "                \n",
    "                iterations += 1  \n",
    "                current_time = time.time()\n",
    "                \n",
    "            y0 = y.copy()\n",
    "\n",
    "        return x, (np.linalg.norm(((x-true_x)))**2)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ours Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class ours_algorithms_time:\n",
    "\n",
    "    def soft_thresholding(self, x, threshold):\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "    def PG_RR(self, initial_x, A, y, lambda_, true_x, r, max_duration, gamma, C, momentum):\n",
    "\n",
    "        start_time = time.time()  \n",
    "        current_time = start_time\n",
    "        iterations = 0  \n",
    "\n",
    "        x = initial_x.copy()\n",
    "        n = len(y)\n",
    "        velocity = np.zeros_like(x)\n",
    "        y0 = y.copy()\n",
    "\n",
    "        while (current_time - start_time) < max_duration:  \n",
    "            # y0+=C*(r**(iterations+1))\n",
    "            y0 += C*r\n",
    "\n",
    "            permuted_indices = np.random.permutation(n)\n",
    "            \n",
    "            for i in permuted_indices:\n",
    "                gradient = 2 * A[i].T @ (A[i] @ x - y0[i])\n",
    "                velocity = momentum * velocity + gamma * gradient\n",
    "                x = self.soft_thresholding(x - velocity, gamma * lambda_)\n",
    "                \n",
    "                iterations += 1  \n",
    "                current_time = time.time()\n",
    "            y0 = y.copy()\n",
    "\n",
    "        return x, (np.linalg.norm(((x-true_x)))**2)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PG algorithm\n",
    "pg_time=pg_algorithms_time()\n",
    "\n",
    "# B-PG algorithm\n",
    "bpg_time = bpg_algorithms_time()\n",
    "\n",
    "# SPG algorithm\n",
    "spg_time = spg_algorithm_time()\n",
    "\n",
    "# ADMM algorithm\n",
    "admm_time = admm_algorithm_time()\n",
    "\n",
    "# PGRR algorithm\n",
    "pgrr_time=pgrr_algorithm_time()\n",
    "\n",
    "# ours algorithm\n",
    "ours_time = ours_algorithms_time()\n",
    "\n",
    "max_duration = 55\n",
    "C = 0 # 3 5 20 30 \n",
    "\n",
    "pg_x,pg_errors= pg_time.proximal_gradient_descent(x, A, y, lambda_, true_x, r, max_duration, 1.6e-7, C)\n",
    "bpg_x,bpg_errors = bpg_time.block_proximal_gradient(x, A, y, lambda_, true_x, r, max_duration, 6e-8, C)\n",
    "spg_x,spg_errors= spg_time.run_spg(x, A, y, lambda_, true_x, r, max_duration, 8.8e-8, C)\n",
    "admm_x,admm_errors = admm_time.admm_solver(x, A, y, lambda_, true_x, r, max_duration, 7e5, C)\n",
    "pg_rr_x,pg_rr_errors= pgrr_time.PG_RR(x, A, y, lambda_, true_x, r, max_duration, 6.5e-8, C)\n",
    "ours_x,ours_errors= ours_time.PG_RR(x, A, y, lambda_, true_x, r, max_duration, 6.5e-8, C, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('', np.array([pg_errors,bpg_errors,spg_errors,admm_errors,pg_rr_errors,ours_errors]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ael",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
